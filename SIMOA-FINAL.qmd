---
title: "SIMOA FINAL"
author: "PO Couture"
format: html
editor: visual
---

## SIMOA FINAL

\*\*\* Planned Analyses \*\*\* All statistical analyses will be conducted using multiply imputed data to account for missingness across variables. Multiple imputation is used to reduce bias and improve statistical power by generating plausible values based on the observed data distribution. The imputation model includes all variables of interest, consistent with current best practices for handling missing data in psychological and health research.

Group Comparisons To examine whether older adults who discontinued their use of benzodiazepine receptor agonists (BZRA) differ from those who continued use, a series of independent samples t-tests will be conducted. These analyses will compare the two groups on a range of psychological and personality variables, including:

BFI-10 (Big Five Inventory - 10-item short form),

SURPS (Substance Use Risk Profile Scale),

PHQ-2 (Patient Health Questionnaire – depression screener),

OSSS-3 (Oslo Social Support Scale),

DBAS-16 (Dysfunctional Beliefs and Attitudes about Sleep),

CISS-21 (Coping Inventory for Stressful Situations).

Additional between-group comparisons will be conducted on demographic and health-related variables (e.g., age, gender, sleep quality, comorbidities) to identify any systematic differences that may be associated with BZRA cessation status.

\*\*\* Predictive Modelling \*\*\* To identify the most important variables associated with successful BZRA cessation, a Random Forest classification model will be employed. Random Forest is a non-parametric ensemble machine learning method that handles complex interactions and non-linear relationships, and is robust to multicollinearity and overfitting.

The Random Forest model will be trained using all personality, psychological, demographic, and health-related variables as predictors, with BZRA cessation (yes/no) as the outcome. Variable importance scores will be used to rank predictors based on their contribution to classification accuracy.

\*\*\* Model Confirmation \*\*\* To validate the findings from the Random Forest model, a logistic regression analysis will be conducted using the top predictors identified by the Random Forest. This traditional regression model will allow for the estimation of effect sizes (odds ratios) and the statistical significance of each variable’s unique contribution to BZRA cessation. Confidence intervals and p-values will be reported, and model diagnostics will be used to assess model fit.

Together, this multi-step analytic strategy aims to both explore and confirm key predictors of BZRA discontinuation among older adults, leveraging the strengths of both machine learning and traditional inferential statistics.

## Data Loading, Screening and Packages

In this section I am loading all the necessary packages for my analysis and loading in the data.

```{r}
#| label: Data Loading and Packages

# Installing and Loading Packages
install.packages("MissMech")
install.packages("randomForest")

library(dplyr)
library(effsize)
library(ggplot2)
library(mice)
library(MissMech)
library(naniar)
library(randomForest)
library(readr)
library(stringr)
library(tidyverse)
library(VIM)

# Data Loading
SIMOA_Report <- read_csv("SIMOA Report.csv")
View(SIMOA_Report)

```

Below I am creating a new object containing only the participants we can confirm are \>= 65 and are using a BZRA by answering which specific BZRA they are using. This is to ensure they are not using other medications that may have sedative effects such as antihistamines or SSRI's.

```{r}
#| label: Creating new object 

Dataset <- SIMOA_Report %>%
  # Apply filtering based on age_cat and age
  filter(
    age_cat == 1 | (age_cat == 0 & age >= 65)
  ) %>%
  # Apply filtering to keep only rows where any of c_sp___1 to c_sp___14 == 1
  filter(
    rowSums(select(., starts_with("c_sp___"))[, 1:14] == 1, na.rm = TRUE) > 0
  )
```

## Calculating Subscale Scores

This section will calculate all the subscale scores for BFI-10, SURPS, DBAS-16, and CISS-21

```{r}
#| label: BFI-10 Subscale

# First, let's create a working copy
data_processed <- Dataset
# ============================================================================
# STEP 1: REVERSE CODE PERSONALITY ITEMS (1-5 scale)
# ============================================================================
# Items to reverse: reserved, find_fault, lazy, relaxed, few_interests
# Formula: reversed_score = 6 - original_score

data_processed <- data_processed %>%
  mutate(
    reserved_rev = 6 - reserved,
    find_fault_rev = 6 - find_fault,
    lazy_rev = 6 - lazy,
    relaxed_rev = 6 - relaxed,
    few_interests_rev = 6 - few_interests
  )

# Verifying Reverse-Coding -- ALL GOOD!!
# Check first 10 rows side by side
data_processed %>%
  select(reserved, reserved_rev, find_fault, find_fault_rev, lazy, lazy_rev, 
         relaxed, relaxed_rev, few_interests, few_interests_rev) %>%
  head(10)

# Verify the math: original + reversed should equal 6
data_processed %>%
  mutate(
    reserved_sum = reserved + reserved_rev,
    find_fault_sum = find_fault + find_fault_rev,
    lazy_sum = lazy + lazy_rev,
    relaxed_sum = relaxed + relaxed_rev,
    few_interests_sum = few_interests + few_interests_rev
  ) %>%
  select(ends_with("_sum")) %>%
  summary()

# ============================================================================
# STEP 2: CREATE PERSONALITY TOTAL SCORES
# ============================================================================
data_processed <- data_processed %>%
  mutate(
    # Extraversion: reserved (reversed) + outgoing
    Extraversion = reserved_rev + outgoing,
    
    # Agreeableness: trusting + find_fault (reversed)
    Agreeableness = trusting + find_fault_rev,
    
    # Conscientiousness: lazy (reversed) + thorough
    Conscientiousness = lazy_rev + thorough,
    
    # Neuroticism: relaxed (reversed) + nervous
    Neuroticism = relaxed_rev + nervous,
    
    # Openness: few_interests (reversed) + imagination
    Openness = few_interests_rev + imagination
  )

```

```{r}
#| label: DBAS-16 Subscale

# ============================================================================
# STEP 3: CREATE DBAS TOTAL SCORES (0-10 scale)
# ============================================================================
data_processed <- data_processed %>%
  mutate(
    # Consequences: dbas_5, dbas_7, dbas_9, dbas_12, dbas_16
    DBAS_Consequences = dbas_5 + dbas_7 + dbas_9 + dbas_12 + dbas_16,
    
    # Worry/Helplessness: dbas_3, dbas_4, dbas_8, dbas_10, dbas_11, dbas_14
    DBAS_Worry_Helplessness = dbas_3 + dbas_4 + dbas_8 + dbas_10 + dbas_11 + dbas_14,
    
    # Expectations: dbas_1, dbas_2
    DBAS_Expectations = dbas1 + dbas_2,
    
    # Medications: dbas_6, dbas_13, dbas_15
    DBAS_Medications = dbas_6 + dbas_13 + dbas_15
  )
```

```{r}
#| label: SURPS Subscale

# ============================================================================
# STEP 4: CREATE SURPS TOTAL SCORES (1-4 scale)
# ============================================================================
# First, reverse code SURPS Hopelessness items (all except surps17)
# Formula for 1-4 scale: reversed_score = 5 - original_score
data_processed <- data_processed %>%
  mutate(
    surps1_rev = 5 - surps1,
    surps4_rev = 5 - surps4,
    surps7_rev = 5 - surps7,
    surps13_rev = 5 - surps13,
    surps20_rev = 5 - surps20,
    surps23_rev = 5 - surps23
    # Note: surps17 is NOT reversed
  )

# Verifying Reverse-Coding -- ALL GOOD!!
data_processed %>%
  select(surps1, surps1_rev, surps4, surps4_rev, surps7, surps7_rev) %>%
  head(10) %>%
  mutate(
    check1 = surps1 + surps1_rev,
    check4 = surps4 + surps4_rev,
    check7 = surps7 + surps7_rev
  )

# Quick verification - all sums should equal 5
cat("All sums should equal 5:\n")
print(unique(data_processed$surps1 + data_processed$surps1_rev))
print(unique(data_processed$surps4 + data_processed$surps4_rev))
print(unique(data_processed$surps7 + data_processed$surps7_rev))

# Create SURPS total scores
data_processed <- data_processed %>%
  mutate(
    # Impulsivity: surps2, surps5, surps11, surps15, surps22
    SURPS_Impulsivity = surps2 + surps5 + surps11 + surps15 + surps22,
    
    # Sensation Seeking: surps3, surps6, surps9, surps12, surps16, surps19
    SURPS_Sensation_Seeking = surps3 + surps6 + surps9 + surps12 + surps16 + surps19,
    
    # Hopelessness: surps1(rev), surps4(rev), surps7(rev), surps13(rev), surps17, surps20(rev), surps23(rev)
    SURPS_Hopelessness = surps1_rev + surps4_rev + surps7_rev + surps13_rev + surps17 + surps20_rev + surps23_rev,
    
    # Anxiety Sensitivity: surps8, surps10, surps14, surps18, surps21
    SURPS_Anxiety_Sensitivity = surps8 + surps10 + surps14 + surps18 + surps21
  )
```

```{r}
#| label: CISS-21 Subscales

# ============================================================================
# STEP 5: CREATE CISS-21 TOTAL SCORES (1-5 scale)
# ============================================================================
data_processed <- data_processed %>%
  mutate(
    # Avoidance Style: ciss1, ciss4, ciss7, ciss9, ciss15, ciss18, ciss21
    CISS_Avoidance_Style = ciss1 + ciss4 + ciss7 + ciss9 + ciss15 + ciss18 + ciss21,
    
    # Task Style: ciss2, ciss6, ciss8, ciss11, ciss13, ciss16, ciss19
    CISS_Task_Style = ciss2 + ciss6 + ciss8 + ciss11 + ciss13 + ciss16 + ciss19,
    
    # Emotional Style: ciss3, ciss5, ciss10, ciss12, ciss14, ciss17, ciss20
    CISS_Emotional_Style = ciss3 + ciss5 + ciss10 + ciss12 + ciss14 + ciss17 + ciss20
  )
```

```{r}
#| label: Final Dataset
# ============================================================================
# STEP 6: CREATE FINAL DATASET WITH TOTAL SCORES AND DEMOGRAPHIC VARIABLES
# ============================================================================
# Select all total scores plus additional variables and demographics for imputation (27 variables total)
final_dataset <- data_processed %>%
  select(
    # DBAS scales (4 variables)
    DBAS_Consequences,
    DBAS_Worry_Helplessness,
    DBAS_Expectations,
    DBAS_Medications,
    # Personality scales (5 variables)
    Extraversion,
    Agreeableness,
    Conscientiousness,
    Neuroticism,
    Openness,
    # SURPS scales (4 variables)
    SURPS_Impulsivity,
    SURPS_Sensation_Seeking,
    SURPS_Hopelessness,
    SURPS_Anxiety_Sensitivity,
    # CISS scales (3 variables)
    CISS_Avoidance_Style,
    CISS_Task_Style,
    CISS_Emotional_Style,
    # Additional variables (2 variables)
    osss_3_score,
    phq2_score,
    # Demographic variables (9 variables)
    age,
    sex,
    gender,
    prov_terr,
    education,
    employment,
    driving_freq,
    income
  )
```

```{r}
#| label: Checking Calculations
# ============================================================================
# STEP 7: VERIFICATION - CHECK YOUR CALCULATIONS
# ============================================================================
# Display summary statistics to verify calculations
cat("=== VERIFICATION: Summary of Final Variables ===\n")
summary(final_dataset)

# Check for any missing values before MICE
cat("\n=== Missing Values Count ===\n")
sapply(final_dataset, function(x) sum(is.na(x)))

# Check range of scores to ensure reverse coding worked
cat("\n=== Score Ranges (to verify calculations) ===\n")
cat("Big Five Personality (should be 2-10):\n")
cat("  Extraversion range:", range(final_dataset$Extraversion, na.rm = TRUE), "\n")
cat("  Agreeableness range:", range(final_dataset$Agreeableness, na.rm = TRUE), "\n")
cat("  Conscientiousness range:", range(final_dataset$Conscientiousness, na.rm = TRUE), "\n")
cat("  Neuroticism range:", range(final_dataset$Neuroticism, na.rm = TRUE), "\n")
cat("  Openness range:", range(final_dataset$Openness, na.rm = TRUE), "\n")

cat("\nSURPS scales:\n")
cat("  Impulsivity range (5 items, 1-4 scale, should be 5-20):", range(final_dataset$SURPS_Impulsivity, na.rm = TRUE), "\n")
cat("  Sensation Seeking range (6 items, 1-4 scale, should be 6-24):", range(final_dataset$SURPS_Sensation_Seeking, na.rm = TRUE), "\n")
cat("  Hopelessness range (7 items, 1-4 scale, should be 7-28):", range(final_dataset$SURPS_Hopelessness, na.rm = TRUE), "\n")
cat("  Anxiety Sensitivity range (5 items, 1-4 scale, should be 5-20):", range(final_dataset$SURPS_Anxiety_Sensitivity, na.rm = TRUE), "\n")

cat("\nCISS scales:\n")
cat("  Avoidance Style range (7 items, 1-5 scale, should be 7-35):", range(final_dataset$CISS_Avoidance_Style, na.rm = TRUE), "\n")
cat("  Task Style range (7 items, 1-5 scale, should be 7-35):", range(final_dataset$CISS_Task_Style, na.rm = TRUE), "\n")
cat("  Emotional Style range (7 items, 1-5 scale, should be 7-35):", range(final_dataset$CISS_Emotional_Style, na.rm = TRUE), "\n")
```

## Multiple Imputation

```{r}
#| label: Multiple Imputation
# ============================================================================
# STEP 8: APPLY MICE IMPUTATION
# ============================================================================
# Set seed for reproducibility
set.seed(123)

# Check data types and prepare for MICE
# MICE will automatically detect categorical vs continuous variables
# But let's ensure proper factor coding for categorical variables
cat("=== PREPARING DATA FOR MICE IMPUTATION ===\n")
cat("Checking data types...\n")

# Convert categorical variables to factors if they aren't already
categorical_vars <- c("sex", "gender", "prov_terr", "education", "employment", "driving_freq", "income")

for(var in categorical_vars) {
  if(var %in% names(final_dataset)) {
    if(!is.factor(final_dataset[[var]])) {
      final_dataset[[var]] <- as.factor(final_dataset[[var]])
      cat(paste("Converted", var, "to factor\n"))
    }
  }
}

# Display data types
str(final_dataset)

# Apply MICE imputation
cat("\n=== RUNNING MICE IMPUTATION ===\n")
cat("This may take a moment with 27 variables...\n")

mice_result <- mice(final_dataset, m = 5, method = 'pmm', printFlag = FALSE)

# Get the completed dataset (using the first imputation)
final_dataset_imputed <- complete(mice_result, 1)

# Display summary of imputed dataset
cat("\n=== FINAL IMPUTED DATASET SUMMARY ===\n")
summary(final_dataset_imputed)

# Check that there are no missing values after imputation
cat("\n=== Missing Values After MICE ===\n")
sapply(final_dataset_imputed, function(x) sum(is.na(x)))

cat("\n=== PROCESS COMPLETE ===\n")
cat("Your final dataset 'final_dataset_imputed' contains 27 variables and is ready for analysis.\n")
cat("Variables included:\n")
cat("  - 4 DBAS scales\n")
cat("  - 5 Big Five personality dimensions\n")
cat("  - 4 SURPS scales\n")
cat("  - 3 CISS coping styles\n")
cat("  - 2 additional variables (osss_3_score, phq2_score)\n")
cat("  - 9 demographic variables (age, sex, gender, prov_terr, education, employment, driving_freq, income)\n")
cat("\nIMPORTANT NOTES:\n")
cat("1. You now have 27 variables total, which is well above the 10-15 recommendation.\n")
cat("2. For your random forest model, you'll need to select the most important variables.\n")
cat("3. Consider creating separate models or using variable selection techniques.\n")
cat("4. The demographic variables will improve imputation quality but may not all be needed in your final model.\n")

# Optional: Create a subset with just the main scales for modeling
cat("\n=== CREATING SUBSET FOR MODELING ===\n")
modeling_dataset <- final_dataset_imputed %>%
  select(
    # Main scales only (16 variables)
    DBAS_Consequences, DBAS_Worry_Helplessness, DBAS_Expectations, DBAS_Medications,
    Extraversion, Agreeableness, Conscientiousness, Neuroticism, Openness,
    SURPS_Impulsivity, SURPS_Sensation_Seeking, SURPS_Hopelessness, SURPS_Anxiety_Sensitivity,
    CISS_Avoidance_Style, CISS_Task_Style, CISS_Emotional_Style
  )

cat("Created 'modeling_dataset' with 16 main psychological scales (no demographics).\n")
cat("This may be more appropriate for your random forest model.\n")
```

## Group Comparison

```{r}
#| label: Data Preparation
# Merge outcome variable with imputed dataset
analysis_dataset <- final_dataset_imputed %>%
  mutate(scrn_stopped_bzra = Dataset$scrn_stopped_bzra)

# Remove cases with missing outcome variable
analysis_dataset <- analysis_dataset %>%
  filter(!is.na(scrn_stopped_bzra))

# Check the merge worked
cat("=== DATASET MERGE CHECK ===\n")
cat("Analysis dataset size:", nrow(analysis_dataset), "\n")
cat("Outcome variable distribution:\n")
table(analysis_dataset$scrn_stopped_bzra, useNA = "always")

```

```{r}
#| label: Setup

# Get variable names (excluding outcome)
vars_to_compare <- names(analysis_dataset)[names(analysis_dataset) != "scrn_stopped_bzra"]

# Create results dataframe
results <- data.frame(
  Variable = character(),
  Type = character(),
  Effect_Size = numeric(),
  Effect_Magnitude = character(),
  OR_or_Mean_Diff = numeric(),
  CI_Lower = numeric(),
  CI_Upper = numeric(),
  P_Value = numeric(),
  stringsAsFactors = FALSE
)
```

```{r}
#| label: Continous Variables

cat("\n=== CONTINUOUS VARIABLES: COHEN'S D AND MEAN DIFFERENCES ===\n")
for(var in vars_to_compare) {
  if(is.numeric(analysis_dataset[[var]])) {
    cat(sprintf("\n--- %s ---\n", var))
    
    # Manual Cohen's d calculation (using your working method)
    group0 <- analysis_dataset[analysis_dataset$scrn_stopped_bzra == 0, var]
    group1 <- analysis_dataset[analysis_dataset$scrn_stopped_bzra == 1, var]
    
    # Remove NAs
    group0 <- group0[!is.na(group0)]
    group1 <- group1[!is.na(group1)]
    
    # Calculate pooled standard deviation
    pooled_sd <- sqrt(((length(group0)-1)*sd(group0)^2 + 
                       (length(group1)-1)*sd(group1)^2) / 
                      (length(group0) + length(group1) - 2))
    
    # Calculate Cohen's d
    cohens_d <- (mean(group1) - mean(group0)) / pooled_sd
    
    # Determine magnitude
    magnitude <- case_when(
      abs(cohens_d) < 0.2 ~ "negligible",
      abs(cohens_d) < 0.5 ~ "small",
      abs(cohens_d) < 0.8 ~ "medium",
      TRUE ~ "large"
    )
    
    # T-test for mean difference and CI
    t_result <- t.test(analysis_dataset[[var]] ~ analysis_dataset$scrn_stopped_bzra)
    # Fix: Calculate mean difference correctly (group1 - group0)
    mean_diff <- t_result$estimate[2] - t_result$estimate[1]  # Or use: -diff(t_result$estimate)
    
    # Group means (fix the printing issue)
    group_means <- analysis_dataset %>%
      group_by(scrn_stopped_bzra) %>%
      summarise(mean = mean(!!sym(var), na.rm = TRUE), .groups = "drop")
    
    cat(sprintf("Cohen's d: %.3f (%s)\n", cohens_d, magnitude))
    cat(sprintf("Mean difference (Group1 - Group0): %.3f (95%% CI: %.3f to %.3f)\n", 
                mean_diff, t_result$conf.int[1], t_result$conf.int[2]))
    cat("Group means:\n")
    for(i in 1:nrow(group_means)) {
      cat(sprintf("  Group %d: %.3f\n", group_means$scrn_stopped_bzra[i], group_means$mean[i]))
    }
    cat("\n")
    
    # Add to results
    results <- rbind(results, data.frame(
      Variable = var,
      Type = "Continuous",
      Effect_Size = cohens_d,
      Effect_Magnitude = magnitude,
      OR_or_Mean_Diff = mean_diff,
      CI_Lower = t_result$conf.int[1],
      CI_Upper = t_result$conf.int[2],
      P_Value = t_result$p.value
    ))
  }
}

```

```{r}
#| label: Categorical Variables

cat("\n=== CATEGORICAL VARIABLES: ODDS RATIOS ===\n")

for(var in vars_to_compare) {
  if(!is.numeric(analysis_dataset[[var]])) {
    cat(sprintf("\n--- %s ---\n", var))
    
    # Create 2x2 table
    cross_tab <- table(analysis_dataset[[var]], analysis_dataset$scrn_stopped_bzra)
    
    # Calculate proportions
    props <- prop.table(cross_tab, margin = 2)
    cat("Proportions by group:\n")
    print(props)
    
    # Calculate OR for each category vs reference
    if(nrow(cross_tab) == 2) {
      # Binary variable
      or_result <- fisher.test(cross_tab)
      cat(sprintf("Odds Ratio: %.3f (95%% CI: %.3f to %.3f)\n", 
                  or_result$estimate, or_result$conf.int[1], or_result$conf.int[2]))
      
      # Add to results
      results <- rbind(results, data.frame(
        Variable = var,
        Type = "Binary",
        Effect_Size = or_result$estimate,
        Effect_Magnitude = ifelse(or_result$estimate > 2 | or_result$estimate < 0.5, "Large", 
                                 ifelse(or_result$estimate > 1.5 | or_result$estimate < 0.67, "Medium", "Small")),
        OR_or_Mean_Diff = or_result$estimate,
        CI_Lower = or_result$conf.int[1],
        CI_Upper = or_result$conf.int[2],
        P_Value = or_result$p.value
      ))
      
    } else {
      # Multi-category variable - calculate OR for each level
      cat("Multi-category variable - ORs calculated against first category:\n")
      
      for(i in 2:nrow(cross_tab)) {
        # Create 2x2 table for this category vs reference
        temp_tab <- rbind(cross_tab[1,], cross_tab[i,])
        or_result <- fisher.test(temp_tab)
        
        cat(sprintf("%s vs %s: OR = %.3f (95%% CI: %.3f to %.3f)\n", 
                    rownames(cross_tab)[i], rownames(cross_tab)[1],
                    or_result$estimate, or_result$conf.int[1], or_result$conf.int[2]))
      }
    }
  }
}

```

```{r}
#| label: Summarry of Meaningful Differences

cat("\n=== SUMMARY OF MEANINGFUL EFFECTS ===\n")

# Continuous variables with meaningful effect sizes
cat("Continuous variables with Cohen's d ≥ 0.3 (small to large effects):\n")
continuous_effects <- results[results$Type == "Continuous" & abs(results$Effect_Size) >= 0.3, ]
if(nrow(continuous_effects) > 0) {
  print(continuous_effects[, c("Variable", "Effect_Size", "Effect_Magnitude", "OR_or_Mean_Diff")])
} else {
  cat("No continuous variables with meaningful effect sizes\n")
}

# Categorical variables with meaningful associations
cat("\nCategorical variables with OR ≤ 0.67 or OR ≥ 1.5 (meaningful associations):\n")
categorical_effects <- results[results$Type == "Binary" & (results$Effect_Size <= 0.67 | results$Effect_Size >= 1.5), ]
if(nrow(categorical_effects) > 0) {
  print(categorical_effects[, c("Variable", "OR_or_Mean_Diff", "CI_Lower", "CI_Upper", "Effect_Magnitude")])
} else {
  cat("No categorical variables with meaningful odds ratios\n")
}

```

## Random Forest Model

In this section I will run the code for my RFM. I will run it once on my imputed data with 27 variables and once with my imputed data with only 18 variables. After that I will run it on my full dataset to see if any of the variables I have not imputed are meaningfully adding to my results.

```{r}
#| label: Dustin Fife Method

set.seed(123)
library(party)
rfmod <- cforest(scrn_stopped_bzra~., data = analysis_dataset); estimates(rfmod)

rfsmall <- cforest(scrn_stopped_bzra~DBAS_Medications + Extraversion + prov_terr + + CISS_Avoidance_Style + SURPS_Anxiety_Sensitivity + driving_freq + sex + SURPS_Sensation_Seeking + SURPS_Impulsivity, data = analysis_dataset); estimates(rfsmall)

## This model below shown to be a better predictor than the one above with a large number of variables. 
set.seed(123)
rfsmall_clean <- cforest(scrn_stopped_bzra ~ DBAS_Medications + driving_freq + sex, 
                        data = analysis_dataset)
estimates(rfsmall_clean)

## To confirm that the above model is the best I will add one variable at a time and re-run the model to see if one varibale can be added
set.seed(123)

# Test CISS_Avoidance_Style (was -0.0002, barely negative)
rf_test1 <- cforest(scrn_stopped_bzra ~ DBAS_Medications + driving_freq + sex + CISS_Avoidance_Style, 
                   data = analysis_dataset)
## Avoidance Style coping appears to be interacting with other predictors since this variable increased in importance since the full model test and the sex importance doubled with this in the model. The OOB error rate was 0.786 
estimates(rf_test1)

# Test prov_terr (geographic variation might be meaningful)
rf_test2 <- cforest(scrn_stopped_bzra ~ DBAS_Medications + driving_freq + sex + prov_terr, 
                   data = analysis_dataset)
## The importance of this variable was still negative and the accuracy dropped substantially (OOB error rate) to 0.774. This confirms that geography is not a useful predictor of BZRA cessation
estimates(rf_test2)

# Test SURPS_Anxiety_Sensitivity (was -0.0002, anxiety-related)
rf_test3 <- cforest(scrn_stopped_bzra ~ DBAS_Medications + driving_freq + sex + SURPS_Anxiety_Sensitivity, 
                   data = analysis_dataset)
## The importance of this variable was still negative and the accuracy dropped slightly (OOB error rate) to 0.779. This confirms that Anxiety Sensitivity is not a used predictor of BZRA cessation
estimates(rf_test3)

###### --------
## Because of the findings above, I will stick with the 3 variable model as it is the most accurate, parsimonious and the less variables I have, the lowr my chances of overfitting
###### --------

# 1. DBAS_Medications - your most important predictor
flexplot(scrn_stopped_bzra ~ DBAS_Medications, data = analysis_dataset, 
         method = "logistic")

# 2. Driving frequency - your second most important predictor
ggplot(analysis_dataset, aes(x = factor(driving_freq), fill = factor(scrn_stopped_bzra))) +
  geom_bar(position = "fill") +
  labs(y = "Proportion", x = "Driving Frequency", 
       fill = "Stopped Benzodiazepines") +
  scale_fill_manual(values = c("0" = "coral", "1" = "turquoise"), 
                    labels = c("0" = "No", "1" = "Yes")) +
  theme_minimal()

# 3. Sex - your third predictor
ggplot(analysis_dataset, aes(x = factor(sex), fill = factor(scrn_stopped_bzra))) +
  geom_bar(position = "fill") +
  labs(y = "Proportion", x = "Sex", 
       fill = "Stopped Benzodiazepines") +
  scale_fill_manual(values = c("0" = "coral", "1" = "turquoise"), 
                    labels = c("0" = "No", "1" = "Yes")) +
  theme_minimal()

install.packages("pdp")
library(pdp)

# Create plots that match your original visualizations
p1 <- partial(rfsmall_clean, pred.var = "DBAS_Medications", plot = TRUE)
p2 <- partial(rfsmall_clean, pred.var = "driving_freq", plot = TRUE)  
p3 <- partial(rfsmall_clean, pred.var = "sex", plot = TRUE)

# Model validation concern: DBAS_Medications partial dependence shows major 
# discrepancy from original linear trend. Raw data showed smooth increase 
# (~0.2 to ~1.0), but RF learned complex non-linear pattern with threshold 
# effect (flat until ~8 meds, sharp increase 10-15, erratic peaks/valleys). 
# Suggests potential overfitting, genuine non-linearity, or interactions 
# with other variables. Requires further investigation.
print(p1)

## Model validation: Partial dependence plots show excellent fit for driving_freq 
# and sex variables. The RF model correctly learned that driving_freq 0 and 2 
# have higher benzodiazepine stopping rates (~0.85, ~0.82) while freq 1 and 3 
# have lower rates. Sex effects are captured perfectly - sex=2 shows higher 
# stopping rates (~0.87 vs ~0.61). These patterns match the original data well.
print(p2)
print(p3)

### DBAS_Medications Model Validation Investigation
# Issue: Partial dependence shows non-linear threshold pattern vs linear raw data trend
# 
# Investigation steps:
# 1. Check variable importance to confirm DBAS_Medications drives predictions
# 2. Examine interactions with driving_freq and sex using 2-way partial plots
# 3. Analyze raw data distribution for natural breakpoints/thresholds
# 4. Assess overfitting via OOB error and model complexity
# 5. Test threshold hypothesis by binning DBAS_Medications (0-5, 6-10, 11-15, 16-20, 21+)
# 6. Compare linear vs non-linear trends across subgroups (sex × driving_freq)
# 
# Expected outcomes:
# - High variable importance confirms DBAS_Medications relevance
# - Interaction plots may explain non-linear pattern
# - Binned analysis tests threshold effect hypothesis
# - Subgroup analysis reveals if linear relationship varies by patient characteristics
# See which variables actually drive your model

#Check Variable Importance
install.packages("party")
library(party)

# Get variable importance
varimp(rfsmall_clean)

# Plot variable importance
plot(varimp(rfsmall_clean))

# Get variable importance
varimp(rfsmall_clean)

# Plot variable importance
plot(varimp(rfsmall_clean))

# Or create a nicer plot
imp_values <- varimp(rfsmall_clean)
print(imp_values)

# Custom plot if you want
library(ggplot2)
imp_df <- data.frame(
  variable = names(imp_values),
  importance = imp_values
)
ggplot(imp_df, aes(x = reorder(variable, importance), y = importance)) +
  geom_col() + coord_flip() + 
  labs(x = "Variable", y = "Variable Importance", title = "Random Forest Variable Importance")

# CRITICAL FINDING: Binned analysis reveals RF model was CORRECT!
# Higher DBAS_Medications = LOWER stopping rates (71% at 0-5 meds vs 15% at 21+ meds)
# Original linear trend was misleading - true relationship is negative with 
# threshold effects. RF successfully learned this complex pattern.
# Model validation: CONFIRMED - threshold effect is real and clinically meaningful.
# Create bins with the correct variable name
analysis_dataset$DBAS_bins <- cut(analysis_dataset$DBAS_Medications, 
                                    breaks = c(-1, 5, 10, 15, 20, 30), 
                                    labels = c("0-5", "6-10", "11-15", "16-20", "21+"))

# Check stopping rates by bins
table(analysis_dataset$DBAS_bins, analysis_dataset$scrn_stopped_bzra)
prop.table(table(analysis_dataset$DBAS_bins, analysis_dataset$scrn_stopped_bzra), margin = 1)
# Possible clinical explanations to explore:
# - Polypharmacy patients have more complex conditions
# - Higher medication burden indicates more severe illness
# - Provider reluctance to deprescribe in complex patients
# - Patient adherence patterns differ by medication burden

# Get detailed performance metrics
confusionMatrix(predict(rfsmall_clean), analysis_dataset$scrn_stopped_bzra)

# Calculate AUC-ROC
library(party)

# Get probabilities for class 1 (stopped benzodiazepines)
rf_probs <- sapply(treeresponse(rfsmall_clean), function(x) x[2])

# Now calculate AUC-ROC
library(pROC)
roc_curve <- roc(analysis_dataset$scrn_stopped_bzra, rf_probs)
auc(roc_curve)
plot(roc_curve, main = "ROC Curve for Random Forest Model")

```

```{r}
# Load required libraries
library(party)
library(randomForest) 
library(caret)
library(ggplot2)
library(dplyr)
library(pROC)

# Set seed for reproducibility
set.seed(123)

# === DIAGNOSTIC FUNCTION ===
trace_warnings <- function(expr) {
  withCallingHandlers(
    expr,
    warning = function(w) {
      if (grepl("row names were found from a short variable", w$message)) {
        cat("WARNING DETECTED AT:\n")
        print(sys.calls())
        cat("\nWARNING MESSAGE:\n", w$message, "\n\n")
      }
    }
  )
}

# === 1. DATA PREPARATION ===
cat("=== DATA PREPARATION ===\n")
str(analysis_dataset)
cat("Outcome distribution:\n")
print(prop.table(table(analysis_dataset$scrn_stopped_bzra)))

# Missing values check
missing_summary <- analysis_dataset %>%
  summarise_all(~sum(is.na(.))) %>%
  gather(variable, missing_count) %>%
  filter(missing_count > 0) %>%
  arrange(desc(missing_count))
if(nrow(missing_summary) > 0) print(missing_summary)

# === 2. TRAIN-TEST SPLIT ===
train_indices <- createDataPartition(analysis_dataset$scrn_stopped_bzra, p = 0.8, list = FALSE)
train_data <- analysis_dataset[train_indices, ]
test_data <- analysis_dataset[-train_indices, ]

cat("Split verification - Training:", round(prop.table(table(train_data$scrn_stopped_bzra))[2], 3),
    "| Test:", round(prop.table(table(test_data$scrn_stopped_bzra))[2], 3), "\n")

# === 3. BASELINE FULL MODEL ===
full_formula <- as.formula(paste("scrn_stopped_bzra ~", 
                                paste(setdiff(names(analysis_dataset), "scrn_stopped_bzra"), 
                                      collapse = " + ")))

rf_full <- cforest(full_formula, data = train_data,
                   controls = cforest_unbiased(ntree = 1000, mtry = 3))

full_importance <- varimp(rf_full, conditional = TRUE)
cat("Full model - Top 10 variables:\n")
print(head(sort(full_importance, decreasing = TRUE), 10))

# === 4. VARIABLE SELECTION PLOT ===
imp_df_full <- data.frame(
  variable = names(full_importance),
  importance = full_importance
) %>%
  arrange(desc(importance)) %>%
  slice_head(n = 15)

full_imp_plot <- ggplot(imp_df_full, aes(x = reorder(variable, importance), y = importance)) +
  geom_col(fill = "steelblue", alpha = 0.7) + 
  coord_flip() + 
  labs(x = "Variable", y = "Variable Importance", 
       title = "Top 15 Variables - Full Model Importance") +
  theme_minimal()
print(full_imp_plot)

# === 5-6. ITERATIVE MODEL REFINEMENT WITH DIAGNOSTICS ===
top_vars <- names(full_importance[full_importance > 0])
cat("\nVariables with positive importance:", length(top_vars), "\n")

# Helper function for clean balancing
balance_data <- function(train_df, vars_to_use) {
  tryCatch({
    library(ROSE)
    rose_input <- train_df[, c(vars_to_use, "outcome_factor")]
    row.names(rose_input) <- NULL  # Clean input
    
    rose_result <- ROSE(outcome_factor ~ ., data = rose_input, seed = 123)
    balanced <- as.data.frame(rose_result$data)
    row.names(balanced) <- NULL  # Clean output
    
    return(list(data = balanced, method = "ROSE"))
  }, error = function(e) {
    # Manual oversampling with clean row names
    stopped <- train_df[train_df$outcome_factor == "Stopped", ]
    still_using <- train_df[train_df$outcome_factor == "Still_Using", ]
    row.names(stopped) <- NULL
    row.names(still_using) <- NULL
    
    n_stopped <- nrow(stopped)
    n_still_using <- nrow(still_using)
    replication_factor <- ceiling(n_still_using / n_stopped)
    
    replicated <- stopped[rep(1:n_stopped, replication_factor), ]
    replicated <- replicated[1:n_still_using, ]
    row.names(replicated) <- NULL
    
    balanced <- rbind(still_using, replicated)
    balanced <- balanced[sample(nrow(balanced)), ]
    row.names(balanced) <- NULL
    
    return(list(data = balanced, method = "manual"))
  })
}

if(length(top_vars) >= 3) {
  # Data preparation
  train_data$scrn_stopped_bzra <- as.numeric(as.character(train_data$scrn_stopped_bzra))
  test_data$scrn_stopped_bzra <- as.numeric(as.character(test_data$scrn_stopped_bzra))
  train_data$outcome_factor <- factor(train_data$scrn_stopped_bzra, levels = c(0, 1), labels = c("Still_Using", "Stopped"))
  test_data$outcome_factor <- factor(test_data$scrn_stopped_bzra, levels = c(0, 1), labels = c("Still_Using", "Stopped"))
  
  # Test different variable counts with diagnostics
  var_counts <- unique(c(3, 5, 7, length(top_vars))[c(3, 5, 7, length(top_vars)) <= length(top_vars)])
  results <- list()
  
  cat("\n=== TESTING MODELS WITH DIAGNOSTICS ===\n")
  
  trace_warnings({
    for(n_vars in var_counts) {
      test_vars <- names(sort(full_importance, decreasing = TRUE))[1:n_vars]
      test_vars <- test_vars[test_vars %in% names(train_data)]
      
      if(length(test_vars) == n_vars) {
        # Balance data with clean approach
        balance_result <- balance_data(train_data, test_vars)
        balanced_data <- balance_result$data
        
        cat("Using", balance_result$method, "balancing for", n_vars, "variables\n")
        
        # Build and test model
        test_formula <- as.formula(paste("outcome_factor ~", paste(test_vars, collapse = " + ")))
        rf_test <- cforest(test_formula, data = balanced_data,
                          controls = cforest_unbiased(ntree = 1000, mtry = floor(sqrt(length(test_vars)))))
        
        test_pred <- predict(rf_test, newdata = test_data)
        test_pred_numeric <- ifelse(test_pred == "Stopped", 1, 0)
        accuracy <- mean(test_pred_numeric == test_data$scrn_stopped_bzra)
        
        cm <- confusionMatrix(test_pred, test_data$outcome_factor, positive = "Stopped")
        
        results[[as.character(n_vars)]] <- list(
          variables = test_vars,
          accuracy = accuracy,
          sensitivity = unname(cm$byClass["Sensitivity"]),
          specificity = unname(cm$byClass["Specificity"]),
          balanced_accuracy = unname(cm$byClass["Balanced Accuracy"])
        )
        
        cat(n_vars, "variables - Accuracy:", round(accuracy, 3), 
            "| Sensitivity:", round(cm$byClass["Sensitivity"], 3),
            "| Specificity:", round(cm$byClass["Specificity"], 3), "\n")
      }
    }
  })
  
  # Find best model
  best_balanced_acc <- 0
  best_n_vars <- 3
  
  for(n in names(results)) {
    if(!is.na(results[[n]]$balanced_accuracy) && results[[n]]$balanced_accuracy > best_balanced_acc) {
      best_balanced_acc <- results[[n]]$balanced_accuracy
      best_n_vars <- as.numeric(n)
    }
  }
  
  cat("\nBest model uses", best_n_vars, "variables with balanced accuracy:", round(best_balanced_acc, 3), "\n")

  # === 7-8. FINAL MODEL ===
  final_vars <- names(sort(full_importance, decreasing = TRUE))[1:best_n_vars]
  final_vars <- final_vars[final_vars %in% names(train_data)]
  
  cat("\nFinal model variables:", paste(final_vars, collapse = ", "), "\n")
  
  # Build final model with diagnostics
  trace_warnings({
    balance_result <- balance_data(train_data, final_vars)
    balanced_final_data <- balance_result$data
    cat("Final model using", balance_result$method, "balancing\n")
    
    final_formula <- as.formula(paste("outcome_factor ~", paste(final_vars, collapse = " + ")))
    rf_final <- cforest(final_formula, data = balanced_final_data,
                       controls = cforest_unbiased(ntree = 1000, mtry = floor(sqrt(length(final_vars)))))
  })
  
  # Final evaluation
  final_importance <- varimp(rf_final, conditional = TRUE)
  final_test_pred <- predict(rf_final, newdata = test_data)
  final_test_pred_numeric <- ifelse(final_test_pred == "Stopped", 1, 0)
  final_cm <- confusionMatrix(final_test_pred, test_data$outcome_factor, positive = "Stopped")
  
  # ROC Analysis
  final_auc <- tryCatch({
    tree_responses <- treeresponse(rf_final, newdata = test_data)
    final_probs <- sapply(tree_responses, function(x) {
      if("Stopped" %in% names(x)) return(as.numeric(x["Stopped"]))
      if(length(x) >= 2) return(as.numeric(x[2]))
      return(0.5)
    })
    final_roc <- roc(test_data$scrn_stopped_bzra, final_probs, quiet = TRUE)
    auc(final_roc)
  }, error = function(e) NA)
  
  # === 9. VISUALIZATION ===
  # Variable importance plot
  final_imp_df <- data.frame(
    variable = names(final_importance[final_importance > 0]),
    importance = final_importance[final_importance > 0]
  )
  
  if(nrow(final_imp_df) > 0) {
    final_imp_plot <- ggplot(final_imp_df, aes(x = reorder(variable, importance), y = importance)) +
      geom_col(fill = "darkgreen", alpha = 0.7) + 
      coord_flip() + 
      labs(x = "Variable", y = "Variable Importance", 
           title = "Final Model Variable Importance") +
      theme_minimal()
    print(final_imp_plot)
  }
  
  # ROC plot if available
  if(!is.na(final_auc)) {
    tryCatch({
      roc_plot <- ggroc(final_roc) +
        geom_abline(intercept = 1, slope = 1, linetype = "dashed", color = "gray") +
        labs(title = paste("ROC Curve (AUC =", round(final_auc, 3), ")"),
             x = "Specificity", y = "Sensitivity") +
        theme_minimal()
      print(roc_plot)
    }, error = function(e) cat("ROC plot failed\n"))
  }
  
  # === 10. FINAL SUMMARY ===
  cat("\n=== FINAL MODEL SUMMARY ===\n")
  cat("Variables:", paste(final_vars, collapse = ", "), "\n")
  cat("Number of variables:", length(final_vars), "\n")
  cat("Test Accuracy:", round(final_cm$overall["Accuracy"], 3), "\n")
  if(!is.na(final_auc)) cat("Test AUC:", round(final_auc, 3), "\n")
  cat("Sensitivity (recall):", round(final_cm$byClass["Sensitivity"], 3), "\n")
  cat("Specificity:", round(final_cm$byClass["Specificity"], 3), "\n")
  cat("Precision:", round(final_cm$byClass["Pos Pred Value"], 3), "\n")
  cat("Balanced Accuracy:", round(final_cm$byClass["Balanced Accuracy"], 3), "\n")
  
  # Practical results
  actual_stopped <- sum(test_data$scrn_stopped_bzra)
  predicted_stopped <- sum(final_test_pred_numeric)
  cat("\nActually stopped:", actual_stopped, "| Predicted to stop:", predicted_stopped, "\n")
  
  if(predicted_stopped > actual_stopped * 0.5) {
    cat("✅ Model predicts reasonable number of stops!\n")
  } else {
    cat("⚠️  Still under-predicting stops\n")
  }
  
} else {
  cat("Error: Insufficient variables with positive importance\n")
}

```


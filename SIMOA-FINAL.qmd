---
title: "SIMOA FINAL"
author: "PO Couture"
format: html
editor: visual
---

## SIMOA FINAL

*** Planned Analyses ***
All statistical analyses will be conducted using multiply imputed data to account for missingness across variables. Multiple imputation is used to reduce bias and improve statistical power by generating plausible values based on the observed data distribution. The imputation model includes all variables of interest, consistent with current best practices for handling missing data in psychological and health research.

Group Comparisons
To examine whether older adults who discontinued their use of benzodiazepine receptor agonists (BZRA) differ from those who continued use, a series of independent samples t-tests will be conducted. These analyses will compare the two groups on a range of psychological and personality variables, including:

BFI-10 (Big Five Inventory - 10-item short form),

SURPS (Substance Use Risk Profile Scale),

PHQ-2 (Patient Health Questionnaire – depression screener),

OSSS-3 (Oslo Social Support Scale),

DBAS-16 (Dysfunctional Beliefs and Attitudes about Sleep),

CISS-21 (Coping Inventory for Stressful Situations).

Additional between-group comparisons will be conducted on demographic and health-related variables (e.g., age, gender, sleep quality, comorbidities) to identify any systematic differences that may be associated with BZRA cessation status.

*** Predictive Modelling ***
To identify the most important variables associated with successful BZRA cessation, a Random Forest classification model will be employed. Random Forest is a non-parametric ensemble machine learning method that handles complex interactions and non-linear relationships, and is robust to multicollinearity and overfitting.

The Random Forest model will be trained using all personality, psychological, demographic, and health-related variables as predictors, with BZRA cessation (yes/no) as the outcome. Variable importance scores will be used to rank predictors based on their contribution to classification accuracy.

*** Model Confirmation ***
To validate the findings from the Random Forest model, a logistic regression analysis will be conducted using the top predictors identified by the Random Forest. This traditional regression model will allow for the estimation of effect sizes (odds ratios) and the statistical significance of each variable’s unique contribution to BZRA cessation. Confidence intervals and p-values will be reported, and model diagnostics will be used to assess model fit.

Together, this multi-step analytic strategy aims to both explore and confirm key predictors of BZRA discontinuation among older adults, leveraging the strengths of both machine learning and traditional inferential statistics.

## Data Loading, Screening and Packages
In this section I am loading all the necessary packages for my analysis and loading in the data.
```{r}
#| label: Data Loading and Packages

# Installing and Loading Packages
install.packages("MissMech")

library(dplyr)
library(ggplot2)
library(mice)
library(MissMech)
library(naniar)
library(readr)
library(stringr)
library(tidyverse)
library(VIM)

# Data Loading
SIMOA_Report <- read_csv("SIMOA Report.csv")
View(SIMOA_Report)

```

Below I am creating a new object containing only the participants we can confirm are >= 65 and are using a BZRA by answering which specific BZRA they are using. This is to ensure they are not using other medications that may have sedative effects such as antihistamines or SSRI's.
```{r}
#| label: Creating new object 

Dataset <- SIMOA_Report %>%
  # Apply filtering based on age_cat and age
  filter(
    age_cat == 1 | (age_cat == 0 & age >= 65)
  ) %>%
  # Apply filtering to keep only rows where any of c_sp___1 to c_sp___14 == 1
  filter(
    rowSums(select(., starts_with("c_sp___"))[, 1:14] == 1, na.rm = TRUE) > 0
  )
```

## Data Cleaning
Below is the section where I will separate the questions into the categories the participants truly saw to avoid higher values for missingness which would be incorrect.
```{r}
#| label: Making Sata Questions Into Factors

# I will begin changing all the 'Select all that apply" questions to factors with 1 indicating an answer was provided and 0 meaning that none of the options were selected by the participant (likely not shown the questions)
# Define all variable prefixes
prefixes <- c(
  "liv_sit___", "sleep_health_con___", "def_insomnia___", "phys_health_con___",
  "ment_health_con___", "alt_nic_use___", "can_use_rsn___", "c_sp___",
  "tablet_split___", "tablet_split_2___", "stop_wdl_exp_2___", "stopping_aids_2___", "other_sub___"
)

# Flatten into a single list of all relevant column names
all_factor_columns <- names(Dataset)[
  sapply(names(Dataset), function(col) any(startsWith(col, prefixes)))
]

# Convert to factors: 1 = "Selected", 0 = "Not selected"
Dataset <- Dataset %>%
  mutate(across(all_of(all_factor_columns), ~ factor(., levels = c(0, 1), labels = c("Not selected", "Selected"))))

# Check that it worked
str(Dataset[all_factor_columns])

saveRDS(Dataset, "cleaned_dataset.rds")
Dataset <- readRDS("cleaned_dataset.rds")

attr(Dataset, "spec") <- NULL

################################################################################
################################################################################
#| label: Removing Columns With Only N/A
# This next section will remove the columns with only N/A values showing that they were not answered by anyone regardless of group and are not needed for our analysis.
# Identify columns with all NA
cols_all_na <- names(Dataset)[colSums(!is.na(Dataset)) == 0]
print(cols_all_na)

# Then remove them
Dataset <- Dataset[, colSums(!is.na(Dataset)) > 0]

################################################################################
################################################################################
#| label: Branching Logic Seperation for the Questions Based on Frequency of Use
# This section will group the questions into which ones were seen by those who answered "2" or using their BZRA more than 1 time per month to the "freq_dich" question.
# === Load REDCap data dictionary ===
dict <- read.csv("Data_Dict.csv", stringsAsFactors = FALSE)

# Rename the branching logic column for easier access
names(dict)[names(dict) == "Branching.Logic..Show.field.only.if...."] <- "branching_logic"

# Normalize branching logic text to lowercase for consistent pattern matching
dict$branching_logic <- tolower(dict$branching_logic)

# === Extract questions based on branching logic ===

# 1. Questions shown only if freq_dich = '2'
freq_use_questions <- dict$Variable...Field.Name[
  grepl("\\[freq_dich\\]\\s*=\\s*'2'", dict$branching_logic)
]

# 2. Questions shown only if freq_dich = '0' OR freq_dich = '1'
not_freq_use_questions <- dict$Variable...Field.Name[
  grepl("\\[freq_dich\\]\\s*=\\s*'0'|\\[freq_dich\\]\\s*=\\s*'1'", dict$branching_logic)
]

# 3. Questions shown to everyone (no branching logic or no freq_dich condition)
common_questions <- dict$Variable...Field.Name[
  is.na(dict$branching_logic) | dict$branching_logic == "" | !grepl("\\[freq_dich\\]", dict$branching_logic)
]

# === OPTIONAL: Filter to variables actually present in your dataset ===
vars_in_dataset <- names(Dataset)

freq_use_questions <- freq_use_questions[freq_use_questions %in% vars_in_dataset]
not_freq_use_questions <- not_freq_use_questions[not_freq_use_questions %in% vars_in_dataset]
common_questions <- common_questions[common_questions %in% vars_in_dataset]

# === Check results ===
cat("Number of freq_use_questions:", length(freq_use_questions), "\n")
cat("Number of not_freq_use_questions:", length(not_freq_use_questions), "\n")
cat("Number of common_questions:", length(common_questions), "\n")

################################################################################
################################################################################
#| label: Branching Logic For All Other Questions
# I need to further the branching logic because men and women were shown separate questions so below is how I have gone about ensuring that missingness is only calculated for the questions participants could have been shown

# Number of participants
n <- nrow(Dataset)

# --- Create logical indicator for other_sub_use upfront ---

# Checkbox columns for other_sub
checkbox_cols <- paste0("other_sub___", 0:8)
checkbox_cols <- checkbox_cols[checkbox_cols %in% colnames(Dataset)]  # ensure columns exist

# Logical vector if "Other" checkbox (___8) is selected
Dataset$other_sub_use <- FALSE
if ("other_sub___8" %in% colnames(Dataset)) {
  Dataset$other_sub_use <- Dataset[["other_sub___8"]] == 1
  Dataset$other_sub_use[is.na(Dataset$other_sub_use)] <- FALSE
}

# --- Function to convert REDCap branching logic to R expression string ---
convert_branching_logic <- function(expr) {
  if (is.na(expr) || expr == "") return(NULL)

  expr_r <- expr %>%
    tolower() %>%
    # Explicitly replace [other_sub_use] first
    str_replace_all("\\[other_sub_use\\]", "Dataset[['other_sub_use']]") %>%
    # Replace [var(num)] with triple underscores
    str_replace_all("\\[([a-z0-9_]+)\\((\\d+)\\)\\]", "Dataset[['\\1___\\2']]") %>%
    # Replace [var]
    str_replace_all("\\[([a-z0-9_]+)\\]", "Dataset[['\\1']]") %>%
    # Fix operators
    str_replace_all("(?<![!<>=])=", "==") %>%
    str_replace_all("!====", "!=") %>%
    str_replace_all("'([0-9]+)'", "\\1") %>%
    str_replace_all("\\band\\b", "&") %>%
    str_replace_all("\\bor\\b", "|")

  expr_r
}

# --- Initialize list to store applicability vectors ---
applicability_list <- vector("list", length = nrow(dict))
names(applicability_list) <- dict$Variable...Field.Name

# --- Loop over all questions and evaluate branching logic ---
for (i in seq_len(nrow(dict))) {
  varname <- dict$Variable...Field.Name[i]
  logic <- dict$branching_logic[i]
  
  if (is.na(logic) || logic == "") {
    # No branching logic → all TRUE
    applicability_list[[varname]] <- rep(TRUE, n)
  } else {
    # Convert to R logical expression string
    expr_r <- convert_branching_logic(logic)
    
    if (is.null(expr_r)) {
      applicability_list[[varname]] <- rep(TRUE, n)
    } else {
      # Evaluate expression safely with improved handling
      applicable_vec <- tryCatch(
        {
          res <- eval(parse(text = expr_r))
          # Convert to logical vector if not already logical
          if (!is.logical(res)) {
            res <- !is.na(res) & res != 0 & res != ""
          }
          # Ensure length matches n
          if (length(res) != n) {
            warning(sprintf("Branching logic for %s returned vector length %d != %d", varname, length(res), n))
            res <- rep(FALSE, n)
          }
          res
        },
        error = function(e) {
          warning(sprintf("Failed to evaluate branching logic for %s: %s", varname, e$message))
          rep(FALSE, n)
        }
      )
      
      # Diagnostic output
      cat(sprintf("Variable: %s, length: %d, class: %s\n", varname, length(applicable_vec), class(applicable_vec)))
      
      # Final check for logical vector of correct length
      if (!is.logical(applicable_vec) || length(applicable_vec) != n) {
        warning(sprintf("Branching logic for %s did not return logical vector of length %d", varname, n))
        applicable_vec <- rep(FALSE, n)
      }
      
      applicability_list[[varname]] <- applicable_vec
    }
  }
}


# Convert list to data.frame or matrix for easy indexing
applicability_matrix <- do.call(cbind, applicability_list)

# Fix colnames for clarity
colnames(applicability_matrix) <- names(applicability_list)

## Description of the applicability matrix
# applicability_matrix[i, j] = TRUE/FALSE if participant i should see question j

##### TESTING TO SEE IF THE CODE WORKED #####
#participants_zero <- which(Dataset$scrn_stopped_bzra == "0")
## Subset responses for confirm_stop_attempt among these participants
#responses_confirm <- Dataset$confirm_stop_attempt[participants_zero]

## Check how many responded (not NA)
#num_answered <- sum(!is.na(responses_confirm))
#num_missing <- sum(is.na(responses_confirm))

#cat("Number of participants with scrn_stopped_bzra == '0':", length(participants_zero), "\n")
#cat("Among them, how many answered confirm_stop_attempt:", num_answered, "\n")
#cat("And how many did NOT answer (missing):", num_missing, "\n")

##### EXTRA CODE THAT MAY BE USEFUL #####
## Example: Check how many participants should see each question
#applicability_counts <- colSums(applicability_matrix)

## Example: Filter questions that apply to at least some participants
#questions_shown <- names(applicability_counts)[applicability_counts > 0]
```

## Missingness
Now that the branching logic has been successfully followed, I can carry on and check missingness and determine if my data is MCAR, MAR, or MNAR which will impact if Multiple Implication is a valuable tool or not. On top of this, if my missingness value is below 5%, it is generally accepted that MI is not needed.

```{r}
#| label: Missingness
# Initialize data frame to store results
missingness_df <- data.frame(
  question = colnames(applicability_matrix),
  n_missing = integer(length(colnames(applicability_matrix))),
  percent_missing = numeric(length(colnames(applicability_matrix))),
  n_applicable = integer(length(colnames(applicability_matrix))),
  stringsAsFactors = FALSE
)

# Loop over each question
for (question in colnames(applicability_matrix)) {
  applicable <- applicability_matrix[, question]           # TRUE/FALSE per respondent
  responses <- Dataset[[question]]                         # Actual values in dataset
  
  n_app <- sum(applicable, na.rm = TRUE)
  
  if (!is.null(applicable) && n_app > 0) {
    responses_applicable <- responses[applicable]
    
    if (length(responses_applicable) > 0) {
      is_missing <- is.na(responses_applicable)
      n_miss <- sum(is_missing)
      pct_miss <- if (n_app > 0) (n_miss / n_app) * 100 else 0
      
      missingness_df$n_missing[missingness_df$question == question] <- n_miss
      missingness_df$n_applicable[missingness_df$question == question] <- n_app
      missingness_df$percent_missing[missingness_df$question == question] <- pct_miss
    } else {
      # No responses for applicable rows
      missingness_df$n_missing[missingness_df$question == question] <- 0
      missingness_df$n_applicable[missingness_df$question == question] <- 0
      missingness_df$percent_missing[missingness_df$question == question] <- 0
    }
  } else {
    # Question never applied to anyone
    missingness_df$n_missing[missingness_df$question == question] <- 0
    missingness_df$n_applicable[missingness_df$question == question] <- 0
    missingness_df$percent_missing[missingness_df$question == question] <- 0
  }
}

# Sort by percentage missing in descending order
missingness_df <- missingness_df[order(-missingness_df$percent_missing), ]

print(missingness_df)

################################################################################
################################################################################

#| label: MCAR Analysis
# Here is the code I will use to examine if my data is MCAR, MAR, or MNAR to help me determine if MI is useful.



```



## Multiple Imputation 
```{r}
#| label: 


```








